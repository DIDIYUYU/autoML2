import pandas as pd
import os
import logging
import json


def download_titanic_data():
    """Загрузка датасета Titanic с Kaggle API"""
    try:
        from kaggle.api.kaggle_api_extended import KaggleApi

        # Пытаемся получить connection из Airflow
        username = None
        key = None
        
        try:
            from airflow.hooks.base import BaseHook
            connection = BaseHook.get_connection('kaggle_default')
            username = connection.login
            key = connection.password
            logging.info("Используем Kaggle credentials из Airflow connection")
        except Exception as e:
            logging.warning(f"Не удалось получить Kaggle credentials из Airflow connection: {e}")
            
        # Fallback на переменные окружения
        if not username or not key:
            username = os.environ.get('KAGGLE_USERNAME')
            key = os.environ.get('KAGGLE_KEY')
            if username and key:
                logging.info("Используем Kaggle credentials из переменных окружения")
            
        # Fallback на файл .kaggle/kaggle.json
        if not username or not key:
            try:
                kaggle_config_path = '/home/airflow/.kaggle/kaggle.json'
                if os.path.exists(kaggle_config_path):
                    with open(kaggle_config_path, 'r') as f:
                        kaggle_config = json.load(f)
                        username = kaggle_config.get('username')
                        key = kaggle_config.get('key')
                        if username and key:
                            logging.info("Используем Kaggle credentials из файла конфигурации")
            except Exception as e:
                logging.warning(f"Не удалось прочитать файл конфигурации Kaggle: {e}")
                
        if not username or not key:
            raise ValueError("Kaggle credentials не найдены. Проверьте Airflow connection 'kaggle_default', переменные окружения KAGGLE_USERNAME/KAGGLE_KEY или файл .kaggle/kaggle.json")

        # Настройка Kaggle API
        kaggle_dir = '/tmp/.kaggle'
        os.makedirs(kaggle_dir, exist_ok=True)

        kaggle_json = {"username": username, "key": key}
        kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')
        with open(kaggle_json_path, 'w') as f:
            json.dump(kaggle_json, f)
        os.chmod(kaggle_json_path, 0o600)
        
        os.environ['KAGGLE_CONFIG_DIR'] = kaggle_dir

        api = KaggleApi()
        api.authenticate()

        # Используем стандартный датасет Titanic от Kaggle
        competition_name = "titanic"
        download_path = "/tmp/titanic_data"
        os.makedirs(download_path, exist_ok=True)
        
        # Скачиваем файлы соревнования
        api.competition_download_files(competition_name, path=download_path, quiet=False)
        
        # Распаковываем архив
        import zipfile
        zip_path = os.path.join(download_path, f"{competition_name}.zip")
        if os.path.exists(zip_path):
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(download_path)
            os.remove(zip_path)  # Удаляем архив после распаковки

        # Ищем файлы train.csv и test.csv
        train_path = os.path.join(download_path, "train.csv")
        test_path = os.path.join(download_path, "test.csv")
        
        if not os.path.exists(train_path) or not os.path.exists(test_path):
            # Fallback: ищем любые CSV файлы
            import glob
            csv_files = glob.glob(f"{download_path}/*.csv")
            logging.info(f"Найденные CSV файлы: {csv_files}")
            
            if len(csv_files) < 2:
                raise FileNotFoundError(f"Недостаточно CSV файлов в {download_path}. Найдено: {csv_files}")
            
            # Берем первые два файла как train и test
            train_path = csv_files[0]
            test_path = csv_files[1]
            logging.info(f"Используем файлы: train={train_path}, test={test_path}")

        # Загружаем данные
        train_df = pd.read_csv(train_path)
        test_df = pd.read_csv(test_path)
        
        # Валидация загруженных данных
        if train_df.empty or test_df.empty:
            raise ValueError("Один из загруженных файлов пустой")
        
        # Проверяем, что в train есть целевая переменная
        if 'Survived' not in train_df.columns:
            logging.warning("Колонка 'Survived' не найдена в тренировочных данных")

        logging.info(f"Данные успешно загружены. Train shape: {train_df.shape}, Test shape: {test_df.shape}")
        logging.info(f"Train columns: {list(train_df.columns)}")
        logging.info(f"Test columns: {list(test_df.columns)}")
        
        return train_df, test_df

    except Exception as e:
        logging.error(f"Ошибка при загрузке данных с Kaggle: {e}")
        logging.info("Пытаемся создать mock данные для тестирования...")
        
        # Создаем простые mock данные для тестирования
        try:
            train_df = create_mock_titanic_data(is_train=True)
            test_df = create_mock_titanic_data(is_train=False)
            logging.info("Mock данные созданы успешно")
            return train_df, test_df
        except Exception as mock_error:
            logging.error(f"Не удалось создать mock данные: {mock_error}")
            raise e  # Возвращаем исходную ошибку

def create_mock_titanic_data(is_train=True, n_samples=100):
    """Создание mock данных Titanic для тестирования"""
    import numpy as np
    
    np.random.seed(42)
    
    data = {
        'PassengerId': range(1, n_samples + 1),
        'Pclass': np.random.choice([1, 2, 3], n_samples),
        'Sex': np.random.choice(['male', 'female'], n_samples),
        'Age': np.random.normal(30, 15, n_samples),
        'SibSp': np.random.choice([0, 1, 2, 3], n_samples, p=[0.6, 0.2, 0.15, 0.05]),
        'Parch': np.random.choice([0, 1, 2, 3], n_samples, p=[0.7, 0.15, 0.1, 0.05]),
        'Fare': np.random.exponential(30, n_samples),
        'Embarked': np.random.choice(['S', 'C', 'Q'], n_samples, p=[0.7, 0.2, 0.1])
    }
    
    # Добавляем целевую переменную только для тренировочных данных
    if is_train:
        # Простая логика: женщины и пассажиры 1 класса выживают чаще
        survival_prob = np.where(
            (data['Sex'] == 'female') | (data['Pclass'] == 1), 
            0.7, 0.3
        )
        data['Survived'] = np.random.binomial(1, survival_prob, n_samples)
    
    df = pd.DataFrame(data)
    
    # Добавляем немного пропущенных значений
    missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)
    df.loc[missing_indices, 'Age'] = np.nan
    
    if 'Embarked' in df.columns:
        missing_embarked = np.random.choice(n_samples, size=2, replace=False)
        df.loc[missing_embarked, 'Embarked'] = np.nan
    
    return df

def save_data_locally(train_df, test_df, path="/tmp/titanic_processed"):
    """Сохранение данных для передачи между задачами"""
    os.makedirs(path, exist_ok=True)
    train_df.to_csv(f"{path}/train.csv", index=False)
    test_df.to_csv(f"{path}/test.csv", index=False)
    
def load_data_from_local(path="/tmp/titanic_processed"):
    """Загрузка данных из локального хранилища"""
    train_df = pd.read_csv(f"{path}/train.csv")
    test_df = pd.read_csv(f"{path}/test.csv")
    return train_df, test_df

